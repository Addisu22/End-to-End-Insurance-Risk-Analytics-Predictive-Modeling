{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe6e608",
   "metadata": {},
   "source": [
    "# Task-3\n",
    "Statistically validate or reject key hypotheses about risk drivers, which will form the basis of our new segmentation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4f4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2577543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Belay\\End-to-End-Insurance-Risk-Analytics-Predictive-Modeling\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "os.chdir(\"..\")  # Go up a directory\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bb73a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set chunk size (number of rows to read at a time)\n",
    "chunk_size = 100000  \n",
    "\n",
    "# Set the separator used in your data.txt file (update if needed)\n",
    "sep = '|'  # for example, '|' or '\\t' or ','\n",
    "\n",
    "# Create an empty list to collect processed chunks (optional)\n",
    "chunks = []\n",
    "\n",
    "# Iterate over chunks of the file\n",
    "for chunk in pd.read_csv(\"Data/MachineLearningRating_v3.txt\", sep=sep, chunksize=chunk_size, engine='python', low_memory=True):\n",
    "    # Example: you can do filtering or processing on each chunk here\n",
    "    # For example, convert date column to datetime\n",
    "    chunk['TransactionMonth'] = pd.to_datetime(chunk['TransactionMonth'], errors='coerce')\n",
    "    \n",
    "    # Example: filter rows if you want, e.g., only claims > 0\n",
    "    # chunk = chunk[chunk['TotalClaims'].astype(float) > 0]\n",
    "    \n",
    "    # Store or process chunk - here we just append to list\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# Combine all chunks (only if you have enough RAM)\n",
    "df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4b86bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth', 'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'mmcode', 'VehicleType', 'RegistrationYear', 'make', 'Model', 'Cylinders', 'cubiccapacity', 'kilowatts', 'bodytype', 'NumberOfDoors', 'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted', 'CrossBorder', 'NumberOfVehiclesInFleet', 'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm', 'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType', 'TotalPremium', 'TotalClaims']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e8babf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Binary flag: Did the policy have any claim?\n",
    "df['ClaimFlag'] = np.where(df['TotalClaims'] > 0, 1, 0)\n",
    "\n",
    "# Claim Severity (avoid division by zero)\n",
    "df['ClaimSeverity'] = df.apply(lambda row: row['TotalClaims'] / row['ClaimFlag'] if row['ClaimFlag'] > 0 else 0, axis=1)\n",
    "\n",
    "# Margin\n",
    "df['Margin'] = df['TotalPremium'] - df['TotalClaims']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca53699",
   "metadata": {},
   "source": [
    "A/B Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec18120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gauteng' 'KwaZulu-Natal' 'Mpumalanga' 'Eastern Cape' 'Western Cape'\n",
      " 'Limpopo' 'North West' 'Free State' 'Northern Cape']\n"
     ]
    }
   ],
   "source": [
    "provinces = df['Province'].unique()\n",
    "print(provinces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76da3898",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_a = df[df['Province'] == 'Gauteng']\n",
    "group_b = df[df['Province'] == 'Limpopo']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a1f9e",
   "metadata": {},
   "source": [
    "Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58dc2629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Frequency test p-value: 0.07991724614845505\n",
      "Accept null hypothesis → there is no significant difference\n"
     ]
    }
   ],
   "source": [
    "# a) Claim Frequency (categorical proportion)\n",
    "# Use Chi-square test or two-proportion z-test:\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "count = np.array([group_a['ClaimFlag'].sum(), group_b['ClaimFlag'].sum()])\n",
    "nobs = np.array([len(group_a), len(group_b)])\n",
    "\n",
    "stat, pval = proportions_ztest(count, nobs)\n",
    "print(f'Claim Frequency test p-value: {pval}')\n",
    "if pval < 0.05:\n",
    "    print(\"Reject null hypothesis → significant difference\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis → there is no significant difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5401cfa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Severity test p-value: 0.047887459586952126\n"
     ]
    }
   ],
   "source": [
    "# b) Claim Severity (continuous data, given claim occurred)\n",
    "# Use t-test to compare means (only for policies with claims):\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Filter policies where claims occurred\n",
    "claims_df = df[df['TotalClaims'] > 0]\n",
    "\n",
    "# Calculate Claim Severity = average claim amount for those policies\n",
    "claim_severity = claims_df['TotalClaims'].mean()\n",
    "\n",
    "# print(\"Claim Severity:\", claim_severity)\n",
    "\n",
    "\n",
    "severity_a = group_a[group_a['ClaimFlag'] == 1]['ClaimSeverity']\n",
    "severity_b = group_b[group_b['ClaimFlag'] == 1]['ClaimSeverity']\n",
    "\n",
    "stat, pval = ttest_ind(severity_a, severity_b, equal_var=False, nan_policy='omit')\n",
    "print(f'Claim Severity test p-value: {pval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a363f566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Margin test p-value: 0.0017399147061173148\n"
     ]
    }
   ],
   "source": [
    "# c) Margin (continuous variable)\n",
    "# Use t-test to compare margins:\n",
    "\n",
    "argin_a = group_a['Margin']\n",
    "margin_b = group_b['Margin']\n",
    "\n",
    "stat, pval = ttest_ind(argin_a, margin_b, equal_var=False, nan_policy='omit')\n",
    "print(f'Margin test p-value: {pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd10b60",
   "metadata": {},
   "source": [
    "For Claim Frequency: use Chi-square test of independence on contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb456502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test for claim frequency across provinces p-value: 5.925510718204678e-19\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "contingency_table = pd.crosstab(df['Province'], df['ClaimFlag'])\n",
    "chi2, pval, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "print(f'Chi-square test for claim frequency across provinces p-value: {pval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6242ef62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA for Claim Severity across provinces p-value: 1.6569173744506565e-07\n"
     ]
    }
   ],
   "source": [
    "# For continuous variables (Claim Severity, Margin): use ANOVA\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "groups = [df[df['Province'] == prov]['ClaimSeverity'].dropna() for prov in provinces]\n",
    "f_stat, pval = f_oneway(*groups)\n",
    "print(f'ANOVA for Claim Severity across provinces p-value: {pval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4209a5",
   "metadata": {},
   "source": [
    "Hypothesis Testing Summary : Interpretation & Business Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a5c78f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claim Frequency by Province: p-value = 5.925510718204678e-19\n",
      "Claim Severity by Province: p-value = 1.6569173744506565e-07\n",
      "Margin by Province: p-value = 0.0011450081247589\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from scipy.stats import ttest_ind, chi2_contingency, f_oneway\n",
    "\n",
    "# Assume df prepared with ClaimFlag, ClaimSeverity, Margin columns\n",
    "\n",
    "# Chi-square test for claim frequency across all provinces\n",
    "contingency_table = pd.crosstab(df['Province'], df['ClaimFlag'])\n",
    "chi2, pval, dof, expected = chi2_contingency(contingency_table)\n",
    "print(f'Claim Frequency by Province: p-value = {pval}')\n",
    "\n",
    "# ANOVA for claim severity across provinces\n",
    "provinces = df['Province'].unique()\n",
    "groups = [df[df['Province'] == prov]['ClaimSeverity'].dropna() for prov in provinces]\n",
    "f_stat, pval = f_oneway(*groups)\n",
    "print(f'Claim Severity by Province: p-value = {pval}')\n",
    "\n",
    "# ANOVA for margin across provinces\n",
    "groups_margin = [df[df['Province'] == prov]['Margin'].dropna() for prov in provinces]\n",
    "f_stat, pval = f_oneway(*groups_margin)\n",
    "print(f'Margin by Province: p-value = {pval}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
