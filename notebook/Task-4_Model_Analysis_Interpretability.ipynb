{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776f44b7",
   "metadata": {},
   "source": [
    "Build and evaluate predictive models that form the core of a dynamic, risk-based pricing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b4cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sys.path.append(os.path.abspath(\"../scripts\"))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b723e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "os.chdir(\"..\")  # Go up a directory\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0887aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e0afb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded total 1000098 rows.\n"
     ]
    }
   ],
   "source": [
    "df = load_data(\"Data/MachineLearningRating_v3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bb0d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate and print RMSE and R² for a regression model.\"\"\"\n",
    "    try:\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        print(f\" {model_name} Performance:\")\n",
    "        print(f\"   RMSE: {rmse:.2f}\")\n",
    "        print(f\"   R²:   {r2:.2f}\")\n",
    "        return {\"Model\": model_name, \"RMSE\": rmse, \"R2\": r2}\n",
    "    except Exception as e:\n",
    "        print(f\" Error evaluating {model_name}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e3c8f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UnderwrittenCoverID', 'PolicyID', 'TransactionMonth', 'IsVATRegistered', 'Citizenship', 'LegalType', 'Title', 'Language', 'Bank', 'AccountType', 'MaritalStatus', 'Gender', 'Country', 'Province', 'PostalCode', 'MainCrestaZone', 'SubCrestaZone', 'ItemType', 'mmcode', 'VehicleType', 'RegistrationYear', 'make', 'Model', 'Cylinders', 'cubiccapacity', 'kilowatts', 'bodytype', 'NumberOfDoors', 'VehicleIntroDate', 'CustomValueEstimate', 'AlarmImmobiliser', 'TrackingDevice', 'CapitalOutstanding', 'NewVehicle', 'WrittenOff', 'Rebuilt', 'Converted', 'CrossBorder', 'NumberOfVehiclesInFleet', 'SumInsured', 'TermFrequency', 'CalculatedPremiumPerTerm', 'ExcessSelected', 'CoverCategory', 'CoverType', 'CoverGroup', 'Section', 'Product', 'StatutoryClass', 'StatutoryRiskType', 'TotalPremium', 'TotalClaims']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29511283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with claims > 0\n",
    "df_claims = df[df['TotalClaims'] > 0].copy()\n",
    "\n",
    "# Drop rows with missing values in predictors or target\n",
    "df_claims.dropna(subset=['TotalClaims', 'CalculatedPremiumPerTerm', 'CustomValueEstimate'], inplace=True)\n",
    "\n",
    "# Select features and target\n",
    "features = ['CalculatedPremiumPerTerm', 'CustomValueEstimate']\n",
    "X = df_claims[features]\n",
    "y = df_claims['TotalClaims']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34e1e198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(X, y):\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        models = {\n",
    "            \"Linear Regression\": LinearRegression(),\n",
    "            \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "            \"XGBoost\": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for name, model in models.items():\n",
    "            model.fit(X_train, y_train)\n",
    "            preds = model.predict(X_test)\n",
    "            perf = evaluate_model_performance(y_test, preds, model_name=name)\n",
    "            results[name] = perf\n",
    "\n",
    "        return models[\"XGBoost\"], X_test, results  # XGBoost assumed best\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model evaluation failed: {e}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e21aca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Linear Regression Performance:\n",
      "   RMSE: 42175.27\n",
      "   R²:   0.25\n",
      " Random Forest Performance:\n",
      "   RMSE: 47313.96\n",
      "   R²:   0.05\n",
      " XGBoost Performance:\n",
      "   RMSE: 49037.09\n",
      "   R²:   -0.02\n",
      "\n",
      " Model Performance Summary:\n",
      " - Linear Regression: RMSE = 42175.27, R² = 0.25\n",
      " - Random Forest: RMSE = 47313.96, R² = 0.05\n",
      " - XGBoost: RMSE = 49037.09, R² = -0.02\n"
     ]
    }
   ],
   "source": [
    "best_model, X_sample, performance_report = evaluate_models(X, y)\n",
    "\n",
    "if performance_report:\n",
    "    print(\"\\n Model Performance Summary:\")\n",
    "    for model, metrics in performance_report.items():\n",
    "        print(f\" - {model}: RMSE = {metrics['RMSE']:.2f}, R² = {metrics['R2']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
